# @package _global_

# Available types of models:
# elastic_net
# xgboost
# lightgbm
# catboost
# widedeep_tab_mlp
# widedeep_tab_resnet
# widedeep_tab_net
# widedeep_tab_transformer
# widedeep_ft_transformer
# widedeep_saint
# widedeep_tab_fastformer
# widedeep_tab_perceiver
# pytorch_tabular_autoint
# pytorch_tabular_tabnet
# pytorch_tabular_node
# pytorch_tabular_category_embedding
# pytorch_tabular_ft_transformer
# pytorch_tabular_tab_transformer
# danet
# nam

# Global params
seed: 1337 # Random seed
task: "regression" # Task type.
model_type: "widedeep_tab_mlp" # Model type. All options are described in the beginning of the file.
target: "Age" # Target column name. Here we perform regression for chronological age.

# Cross-validation params
cv_is_split: True # Perform cross-validation?
cv_n_splits: 5 # Number of splits in cross-validation.
cv_n_repeats: 5 # Number of repeats in cross-validation.

# Optimization metrics params
optimized_metric: "mean_absolute_error" # All metrics listed in src.tasks.metrics.
optimized_mean: "cv_mean" # Optimize mean result across all cross-validation splits? Options: ["", "cv_mean"].
optimized_part: "val" # Optimized data partition. Options: ["val", "tst"].
direction: "min" # Direction of metrics optimization. Options ["min", "max"].

# Run params
max_epochs: 1000 # Maximum number of epochs.
patience: 100 # Number of early stopping epochs.
feature_importance: none # Feature importance method. Options: [none, shap_deep, shap_kernel, shap_tree, native].

# Info params
debug: False # Is Debug?
print_config: False # Print config?
print_model: False # Print model info?
ignore_warnings: True # Ignore warnings?
test_after_training: True # Test after training?

# Directories and files params
project_name: ${model_type}
base_dir: "${oc.env:PROJECT_ROOT}/data"
data_dir: "${base_dir}"
work_dir: "${base_dir}/models/${project_name}"

# SHAP values params
is_shap: False # Calculate SHAP values?
is_shap_save: False # Save SHAP values?
shap_explainer: "Tree" # Type of explainer. Options: ["Tree", "Kernel", "Deep"].
shap_bkgrd: "tree_path_dependent" # Type of background data. Options: ["trn", "all", "tree_path_dependent"].

# Plot params
num_top_features: 10 # Number of most important features to plot
num_examples: 10 # Number of samples to plot some SHAP figures

# Data params
in_dim: 46 # Number of input features
out_dim: 1 # Output dimension (can be more than 1 in multiclass classification problem)
embed_dim: 16 # Default embedding dimension


# Model params
model:
  type: ${model_type}

# Elastic Net params [https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html]
elastic_net:
  alpha: 15.0
  l1_ratio: 0.5
  max_iter: 100000
  tol: 1e-2

# XGBoost params [https://xgboost.readthedocs.io/en/stable/parameter.html]
xgboost:
  booster: 'gbtree'
  learning_rate: 0.05
  max_depth: 5
  gamma: 0
  sampling_method: 'uniform'
  subsample: 1
  objective: 'reg:squarederror'
  verbosity: 0
  eval_metric: 'mae'
  max_epochs: ${max_epochs}
  patience: ${patience}

# LightGBM parameters [https://lightgbm.readthedocs.io/en/latest/Parameters.html]
lightgbm:
  objective: regression
  boosting: gbdt
  learning_rate: 0.042943516804783644
  num_leaves: 31
  device: cpu
  max_depth: -1
  min_data_in_leaf: 9
  feature_fraction: 0.973270293196495
  bagging_fraction: 0.6189923756647909
  bagging_freq: 5
  verbose: -1
  metric: l1
  max_epochs: ${max_epochs}
  patience: ${patience}

# CatBoost params [https://catboost.ai/en/docs/references/training-parameters/]
catboost:
  loss_function: 'MAE'
  learning_rate: 0.03250338619913498
  depth: 4
  min_data_in_leaf: 35
  max_leaves: 31
  task_type: 'CPU'
  verbose: 0
  max_epochs: ${max_epochs}
  patience: ${patience}

# Params for all adapted models from widedeep available here:
# [https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html]
widedeep_tab_mlp:
  _target_: src.models.tabular.widedeep.tab_mlp.WDTabMLPModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  continuous_cols: null
  cont_norm_layer: "batchnorm"
  embed_continuous: False
  cont_embed_dim: 16
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  mlp_hidden_dims:
  - 200
  - 100
  - ${out_dim}
  mlp_activation: 'relu'
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: False

widedeep_tab_resnet:
  _target_: src.models.tabular.widedeep.tab_resnet.WDTabResnetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.07
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  continuous_cols: null
  cont_norm_layer: "batchnorm"
  embed_continuous: False
  cont_embed_dim: 16
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  blocks_dims:
  - 200
  - 100
  - 100
  - ${out_dim}
  blocks_dropout: 0.1
  simplify_blocks: False
  mlp_hidden_dims: null
  mlp_activation: 'relu'
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: False

widedeep_tab_net:
  _target_: src.models.tabular.widedeep.tab_net.WDTabNetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.12476988667170572
  optimizer_weight_decay: 1.216983116182198e-07
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  continuous_cols: null
  cont_norm_layer: "batchnorm"
  embed_continuous: False
  cont_embed_dim: 16
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  n_steps: 2
  attn_dim: 20
  dropout: 0.028354475497966526
  n_glu_step_dependent: 1
  n_glu_shared: 4
  ghost_bn: True
  virtual_batch_size: 128
  momentum: 0.1068950240822053
  gamma: 1.1677939635640748
  epsilon: 1e-15
  mask_type: "entmax"

widedeep_tab_transformer:
  _target_: src.models.tabular.widedeep.tab_transformer.WDTabTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  embed_continuous: True
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: ${embed_dim}
  n_heads: 8
  use_qkv_bias: False
  n_blocks: 4
  attn_dropout: 0.2
  ff_dropout: 0.1
  transformer_activation: "gelu"
  mlp_hidden_dims:
    - 200
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_ft_transformer:
  _target_: src.models.tabular.widedeep.ft_transformer.WDFTTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.009394510896784777
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: 64
  kv_compression_factor: 0.5
  kv_sharing: False
  use_qkv_bias: False
  n_heads: 16
  n_blocks: 2
  attn_dropout: 0.2
  ff_dropout: 0.2
  transformer_activation: "reglu"
  ff_factor: 1.33
  mlp_hidden_dims:
    - 200
    - 100
    - 50
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.05
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_saint:
  _target_: src.models.tabular.widedeep.saint.WDSAINTModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: 8
  use_qkv_bias: False
  n_heads: 2
  n_blocks: 3
  attn_dropout: 0.1
  ff_dropout: 0.2
  transformer_activation: "gelu"
  mlp_hidden_dims:
    - 200
    - 100
    - 50
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_tab_fastformer:
  _target_: src.models.tabular.widedeep.tab_fastformer.WDTabFastFormerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: ${embed_dim}
  n_heads: 8
  use_bias: False
  n_blocks: 4
  attn_dropout: 0.2
  ff_dropout: 0.1
  share_qv_weights: False
  share_weights: False
  transformer_activation: "relu"
  mlp_hidden_dims:
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

widedeep_tab_perceiver:
  _target_: src.models.tabular.widedeep.tab_perceiver.WDTabPerceiverModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  column_idx: null
  cat_embed_input: null
  cat_embed_dropout: 0.1
  use_cat_bias: False
  cat_embed_activation: null
  full_embed_dropout: False
  shared_embed: False
  add_shared_embed: False
  frac_shared_embed: 0.25
  continuous_cols: null
  cont_norm_layer: null
  cont_embed_dropout: 0.1
  use_cont_bias: True
  cont_embed_activation: null
  embed_dim: ${embed_dim}
  n_cross_attns: 1
  n_cross_attn_heads:  4
  n_latents: 16
  latent_dim: 128
  n_latent_heads: 4
  n_latent_blocks: 4
  n_perceiver_blocks: 4
  share_weights: False
  attn_dropout: 0.1
  ff_dropout: 0.1
  transformer_activation: "geglu"
  mlp_hidden_dims:
    - 100
    - ${out_dim}
  mlp_activation: "relu"
  mlp_dropout: 0.1
  mlp_batchnorm: False
  mlp_batchnorm_last: False
  mlp_linear_first: True

# Params for all adapted models from pytorch_tabular available here:
# [https://github.com/manujosephv/pytorch_tabular/tree/main/pytorch_tabular/models]
pytorch_tabular_autoint:
  _target_: src.models.tabular.pytorch_tabular.autoint.PTAutoIntModel
  task: ${task}
  loss_type: L1Loss
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.00992093303237566
  optimizer_weight_decay: 2.472886647072828e-06
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  attn_embed_dim: 64
  num_heads: 1
  num_attn_blocks: 1
  attn_dropouts: 0.1
  has_residuals: true
  embedding_dim: 64
  embedding_dropout: 0.1
  deep_layers: true
  layers: 128-64-32
  activation: ReLU
  dropout: 0.0
  use_batch_norm: false
  batch_norm_continuous_input: false
  attention_pooling: false
  initialization: kaiming

pytorch_tabular_tabnet:
  _target_: src.models.tabular.pytorch_tabular.tabnet.PTTabNetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  n_d: 8
  n_a: 8
  n_steps: 3
  gamma: 1.3
  n_independent: 1
  n_shared: 2
  virtual_batch_size: 128
  mask_type: "sparsemax"

pytorch_tabular_node:
  _target_: src.models.tabular.pytorch_tabular.node.PTNODEModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.2
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  num_layers: 1
  num_trees: 512
  additional_tree_output_dim: 4
  depth: 6
  choice_function: "entmax15"
  bin_function: "sparsemoid"
  max_features: null
  input_dropout: 0.0
  initialize_response: "normal"
  initialize_selection_logits: "uniform"
  threshold_init_beta: 1.0
  threshold_init_cutoff: 1.0
  embed_categorical: False
  embedding_dropout: 0.0

pytorch_tabular_category_embedding:
  _target_: src.models.tabular.pytorch_tabular.category_embedding.PTCategoryEmbeddingModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.05
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  layers: "128-64-32"
  batch_norm_continuous_input: True
  activation: "ReLU"
  embedding_dropout: 0.5
  dropout: 0.5
  use_batch_norm: False
  initialization: "kaiming"

pytorch_tabular_ft_transformer:
  _target_: src.models.tabular.pytorch_tabular.ft_transformer.PTFTTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.05
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  input_embed_dim: 32
  embedding_initialization: "kaiming_uniform"
  embedding_bias: True
  embedding_dropout: 0.1
  share_embedding: False
  share_embedding_strategy: "fraction"
  shared_embedding_fraction: 0.25
  attn_feature_importance: True
  num_heads: 8
  num_attn_blocks: 6
  transformer_head_dim: null
  attn_dropout: 0.1
  add_norm_dropout: 0.1
  ff_dropout: 0.1
  ff_hidden_multiplier: 4
  transformer_activation: "GEGLU"
  out_ff_layers: "128-64-32"
  out_ff_activation: "ReLU"
  out_ff_dropout: 0.0
  use_batch_norm: False
  batch_norm_continuous_input: False
  out_ff_initialization: "kaiming"

pytorch_tabular_tab_transformer:
  _target_: src.models.tabular.pytorch_tabular.tab_transformer.PTTabTransformerModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.05
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  embedding_dims: null
  continuous_cols: null
  categorical_cols: null
  input_embed_dim: 32
  embedding_dropout: 0.1
  share_embedding: False
  share_embedding_strategy: "fraction"
  shared_embedding_fraction: 0.25
  num_heads: 8
  num_attn_blocks: 6
  transformer_head_dim: null
  attn_dropout: 0.1
  add_norm_dropout: 0.1
  ff_dropout: 0.1
  ff_hidden_multiplier: 4
  transformer_activation: "GEGLU"
  out_ff_layers: "128-64-32"
  out_ff_activation: "ReLU"
  out_ff_dropout: 0.0
  use_batch_norm: False
  batch_norm_continuous_input: False
  out_ff_initialization: "kaiming"

# DANet params [https://arxiv.org/abs/2112.02962]
danet:
  _target_: src.models.tabular.danet.danet.DANetModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.09103819498050514
  optimizer_weight_decay: 2.1396873600065802e-05
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  layer_num: 30
  base_outdim: 64
  k: 7
  virtual_batch_size: 256
  drop_rate: 0.03479968662869476

# NAM params [https://github.com/AmrMKayid/nam]
nam:
  _target_: src.models.tabular.nam.nam.NeuralAdditiveModel
  task: ${task}
  loss_type: "L1Loss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 100
  scheduler_gamma: 0.8
  hidden_sizes: []  # [64, 32],
  activation: 'exu'  ## Either `ExU` or `Relu`
  dropout: 0.1
  feature_dropout: 0.1  # 0.5,
  decay_rate: 0.995
  l2_regularization: 0.1
  output_regularization: 0.1
  num_basis_functions: 500
  units_multiplier: 2
  use_dnn: False
  num_units: null
